// Code generated by protoc-gen-gogo. DO NOT EDIT.
// source: tensorflow/core/framework/step_stats.proto

package tensorflow

import (
	fmt "fmt"
	proto "github.com/gogo/protobuf/proto"
	io "io"
	math "math"
	math_bits "math/bits"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.GoGoProtoPackageIsVersion3 // please upgrade the proto package

type AllocatorMemoryUsed struct {
	AllocatorName string `protobuf:"bytes,1,opt,name=allocator_name,json=allocatorName,proto3" json:"allocator_name,omitempty"`
	TotalBytes    int64  `protobuf:"varint,2,opt,name=total_bytes,json=totalBytes,proto3" json:"total_bytes,omitempty"`
	PeakBytes     int64  `protobuf:"varint,3,opt,name=peak_bytes,json=peakBytes,proto3" json:"peak_bytes,omitempty"`
}

func (m *AllocatorMemoryUsed) Reset()         { *m = AllocatorMemoryUsed{} }
func (m *AllocatorMemoryUsed) String() string { return proto.CompactTextString(m) }
func (*AllocatorMemoryUsed) ProtoMessage()    {}
func (*AllocatorMemoryUsed) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e915309f7ed52e5, []int{0}
}
func (m *AllocatorMemoryUsed) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *AllocatorMemoryUsed) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_AllocatorMemoryUsed.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *AllocatorMemoryUsed) XXX_Merge(src proto.Message) {
	xxx_messageInfo_AllocatorMemoryUsed.Merge(m, src)
}
func (m *AllocatorMemoryUsed) XXX_Size() int {
	return m.Size()
}
func (m *AllocatorMemoryUsed) XXX_DiscardUnknown() {
	xxx_messageInfo_AllocatorMemoryUsed.DiscardUnknown(m)
}

var xxx_messageInfo_AllocatorMemoryUsed proto.InternalMessageInfo

func (m *AllocatorMemoryUsed) GetAllocatorName() string {
	if m != nil {
		return m.AllocatorName
	}
	return ""
}

func (m *AllocatorMemoryUsed) GetTotalBytes() int64 {
	if m != nil {
		return m.TotalBytes
	}
	return 0
}

func (m *AllocatorMemoryUsed) GetPeakBytes() int64 {
	if m != nil {
		return m.PeakBytes
	}
	return 0
}

// Output sizes recorded for a single execution of a graph node.
type NodeOutput struct {
	Slot              int32              `protobuf:"varint,1,opt,name=slot,proto3" json:"slot,omitempty"`
	TensorDescription *TensorDescription `protobuf:"bytes,3,opt,name=tensor_description,json=tensorDescription,proto3" json:"tensor_description,omitempty"`
}

func (m *NodeOutput) Reset()         { *m = NodeOutput{} }
func (m *NodeOutput) String() string { return proto.CompactTextString(m) }
func (*NodeOutput) ProtoMessage()    {}
func (*NodeOutput) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e915309f7ed52e5, []int{1}
}
func (m *NodeOutput) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *NodeOutput) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_NodeOutput.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *NodeOutput) XXX_Merge(src proto.Message) {
	xxx_messageInfo_NodeOutput.Merge(m, src)
}
func (m *NodeOutput) XXX_Size() int {
	return m.Size()
}
func (m *NodeOutput) XXX_DiscardUnknown() {
	xxx_messageInfo_NodeOutput.DiscardUnknown(m)
}

var xxx_messageInfo_NodeOutput proto.InternalMessageInfo

func (m *NodeOutput) GetSlot() int32 {
	if m != nil {
		return m.Slot
	}
	return 0
}

func (m *NodeOutput) GetTensorDescription() *TensorDescription {
	if m != nil {
		return m.TensorDescription
	}
	return nil
}

// Time/size stats recorded for a single execution of a graph node.
type NodeExecStats struct {
	// TODO(tucker): Use some more compact form of node identity than
	// the full string name.  Either all processes should agree on a
	// global id (cost_id?) for each node, or we should use a hash of
	// the name.
	NodeName         string                   `protobuf:"bytes,1,opt,name=node_name,json=nodeName,proto3" json:"node_name,omitempty"`
	AllStartMicros   int64                    `protobuf:"varint,2,opt,name=all_start_micros,json=allStartMicros,proto3" json:"all_start_micros,omitempty"`
	OpStartRelMicros int64                    `protobuf:"varint,3,opt,name=op_start_rel_micros,json=opStartRelMicros,proto3" json:"op_start_rel_micros,omitempty"`
	OpEndRelMicros   int64                    `protobuf:"varint,4,opt,name=op_end_rel_micros,json=opEndRelMicros,proto3" json:"op_end_rel_micros,omitempty"`
	AllEndRelMicros  int64                    `protobuf:"varint,5,opt,name=all_end_rel_micros,json=allEndRelMicros,proto3" json:"all_end_rel_micros,omitempty"`
	Memory           []*AllocatorMemoryUsed   `protobuf:"bytes,6,rep,name=memory,proto3" json:"memory,omitempty"`
	Output           []*NodeOutput            `protobuf:"bytes,7,rep,name=output,proto3" json:"output,omitempty"`
	TimelineLabel    string                   `protobuf:"bytes,8,opt,name=timeline_label,json=timelineLabel,proto3" json:"timeline_label,omitempty"`
	ScheduledMicros  int64                    `protobuf:"varint,9,opt,name=scheduled_micros,json=scheduledMicros,proto3" json:"scheduled_micros,omitempty"`
	ThreadId         uint32                   `protobuf:"varint,10,opt,name=thread_id,json=threadId,proto3" json:"thread_id,omitempty"`
	ReferencedTensor []*AllocationDescription `protobuf:"bytes,11,rep,name=referenced_tensor,json=referencedTensor,proto3" json:"referenced_tensor,omitempty"`
}

func (m *NodeExecStats) Reset()         { *m = NodeExecStats{} }
func (m *NodeExecStats) String() string { return proto.CompactTextString(m) }
func (*NodeExecStats) ProtoMessage()    {}
func (*NodeExecStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e915309f7ed52e5, []int{2}
}
func (m *NodeExecStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *NodeExecStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_NodeExecStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *NodeExecStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_NodeExecStats.Merge(m, src)
}
func (m *NodeExecStats) XXX_Size() int {
	return m.Size()
}
func (m *NodeExecStats) XXX_DiscardUnknown() {
	xxx_messageInfo_NodeExecStats.DiscardUnknown(m)
}

var xxx_messageInfo_NodeExecStats proto.InternalMessageInfo

func (m *NodeExecStats) GetNodeName() string {
	if m != nil {
		return m.NodeName
	}
	return ""
}

func (m *NodeExecStats) GetAllStartMicros() int64 {
	if m != nil {
		return m.AllStartMicros
	}
	return 0
}

func (m *NodeExecStats) GetOpStartRelMicros() int64 {
	if m != nil {
		return m.OpStartRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetOpEndRelMicros() int64 {
	if m != nil {
		return m.OpEndRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetAllEndRelMicros() int64 {
	if m != nil {
		return m.AllEndRelMicros
	}
	return 0
}

func (m *NodeExecStats) GetMemory() []*AllocatorMemoryUsed {
	if m != nil {
		return m.Memory
	}
	return nil
}

func (m *NodeExecStats) GetOutput() []*NodeOutput {
	if m != nil {
		return m.Output
	}
	return nil
}

func (m *NodeExecStats) GetTimelineLabel() string {
	if m != nil {
		return m.TimelineLabel
	}
	return ""
}

func (m *NodeExecStats) GetScheduledMicros() int64 {
	if m != nil {
		return m.ScheduledMicros
	}
	return 0
}

func (m *NodeExecStats) GetThreadId() uint32 {
	if m != nil {
		return m.ThreadId
	}
	return 0
}

func (m *NodeExecStats) GetReferencedTensor() []*AllocationDescription {
	if m != nil {
		return m.ReferencedTensor
	}
	return nil
}

type DeviceStepStats struct {
	Device    string           `protobuf:"bytes,1,opt,name=device,proto3" json:"device,omitempty"`
	NodeStats []*NodeExecStats `protobuf:"bytes,2,rep,name=node_stats,json=nodeStats,proto3" json:"node_stats,omitempty"`
}

func (m *DeviceStepStats) Reset()         { *m = DeviceStepStats{} }
func (m *DeviceStepStats) String() string { return proto.CompactTextString(m) }
func (*DeviceStepStats) ProtoMessage()    {}
func (*DeviceStepStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e915309f7ed52e5, []int{3}
}
func (m *DeviceStepStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *DeviceStepStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_DeviceStepStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *DeviceStepStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_DeviceStepStats.Merge(m, src)
}
func (m *DeviceStepStats) XXX_Size() int {
	return m.Size()
}
func (m *DeviceStepStats) XXX_DiscardUnknown() {
	xxx_messageInfo_DeviceStepStats.DiscardUnknown(m)
}

var xxx_messageInfo_DeviceStepStats proto.InternalMessageInfo

func (m *DeviceStepStats) GetDevice() string {
	if m != nil {
		return m.Device
	}
	return ""
}

func (m *DeviceStepStats) GetNodeStats() []*NodeExecStats {
	if m != nil {
		return m.NodeStats
	}
	return nil
}

type StepStats struct {
	DevStats []*DeviceStepStats `protobuf:"bytes,1,rep,name=dev_stats,json=devStats,proto3" json:"dev_stats,omitempty"`
}

func (m *StepStats) Reset()         { *m = StepStats{} }
func (m *StepStats) String() string { return proto.CompactTextString(m) }
func (*StepStats) ProtoMessage()    {}
func (*StepStats) Descriptor() ([]byte, []int) {
	return fileDescriptor_1e915309f7ed52e5, []int{4}
}
func (m *StepStats) XXX_Unmarshal(b []byte) error {
	return m.Unmarshal(b)
}
func (m *StepStats) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	if deterministic {
		return xxx_messageInfo_StepStats.Marshal(b, m, deterministic)
	} else {
		b = b[:cap(b)]
		n, err := m.MarshalToSizedBuffer(b)
		if err != nil {
			return nil, err
		}
		return b[:n], nil
	}
}
func (m *StepStats) XXX_Merge(src proto.Message) {
	xxx_messageInfo_StepStats.Merge(m, src)
}
func (m *StepStats) XXX_Size() int {
	return m.Size()
}
func (m *StepStats) XXX_DiscardUnknown() {
	xxx_messageInfo_StepStats.DiscardUnknown(m)
}

var xxx_messageInfo_StepStats proto.InternalMessageInfo

func (m *StepStats) GetDevStats() []*DeviceStepStats {
	if m != nil {
		return m.DevStats
	}
	return nil
}

func init() {
	proto.RegisterType((*AllocatorMemoryUsed)(nil), "tensorflow.AllocatorMemoryUsed")
	proto.RegisterType((*NodeOutput)(nil), "tensorflow.NodeOutput")
	proto.RegisterType((*NodeExecStats)(nil), "tensorflow.NodeExecStats")
	proto.RegisterType((*DeviceStepStats)(nil), "tensorflow.DeviceStepStats")
	proto.RegisterType((*StepStats)(nil), "tensorflow.StepStats")
}

func init() {
	proto.RegisterFile("tensorflow/core/framework/step_stats.proto", fileDescriptor_1e915309f7ed52e5)
}

var fileDescriptor_1e915309f7ed52e5 = []byte{
	// 596 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x74, 0x53, 0x4d, 0x4f, 0xdb, 0x40,
	0x10, 0xc5, 0x7c, 0xa4, 0xf1, 0x44, 0x94, 0xb0, 0x48, 0xc8, 0x05, 0x61, 0xd2, 0x48, 0x95, 0x42,
	0xab, 0x06, 0x89, 0x4a, 0x2d, 0xd7, 0x22, 0x90, 0x5a, 0x09, 0x28, 0x32, 0xed, 0xd9, 0x5a, 0xbc,
	0x43, 0xb1, 0x58, 0x7b, 0xad, 0xf5, 0x26, 0x14, 0xa9, 0xd7, 0xde, 0xfb, 0xb3, 0x7a, 0xe4, 0xd8,
	0x63, 0x95, 0xfc, 0x89, 0x1e, 0xab, 0x1d, 0x3b, 0xb1, 0x1b, 0xe0, 0x36, 0x7e, 0xfb, 0xe6, 0xf9,
	0xcd, 0xce, 0x5b, 0x78, 0x69, 0x30, 0xcd, 0x95, 0xbe, 0x94, 0xea, 0x66, 0x37, 0x52, 0x1a, 0x77,
	0x2f, 0x35, 0x4f, 0xf0, 0x46, 0xe9, 0xeb, 0xdd, 0xdc, 0x60, 0x16, 0xe6, 0x86, 0x9b, 0xbc, 0x9f,
	0x69, 0x65, 0x14, 0x83, 0x8a, 0xbb, 0xf1, 0xf6, 0xf1, 0x3e, 0x2e, 0xa5, 0x8a, 0xb8, 0x89, 0x55,
	0x1a, 0x0a, 0xcc, 0x23, 0x1d, 0x67, 0xb6, 0x2e, 0x34, 0x36, 0xf6, 0x1e, 0xef, 0x2b, 0x4e, 0xee,
	0xf7, 0x74, 0xbf, 0xc3, 0xda, 0xfb, 0x42, 0x53, 0xe9, 0x13, 0x4c, 0x94, 0xbe, 0xfd, 0x92, 0xa3,
	0x60, 0x2f, 0xe0, 0x29, 0x9f, 0xc0, 0x61, 0xca, 0x13, 0xf4, 0x9c, 0x8e, 0xd3, 0x73, 0x83, 0xe5,
	0x29, 0x7a, 0xca, 0x13, 0x64, 0xdb, 0xd0, 0x32, 0xca, 0x70, 0x19, 0x5e, 0xdc, 0x1a, 0xcc, 0xbd,
	0xf9, 0x8e, 0xd3, 0x5b, 0x08, 0x80, 0xa0, 0x03, 0x8b, 0xb0, 0x2d, 0x80, 0x0c, 0xf9, 0x75, 0x79,
	0xbe, 0x40, 0xe7, 0xae, 0x45, 0xe8, 0xb8, 0x9b, 0x02, 0x9c, 0x2a, 0x81, 0x9f, 0x06, 0x26, 0x1b,
	0x18, 0xc6, 0x60, 0x31, 0x97, 0xca, 0xd0, 0xaf, 0x96, 0x02, 0xaa, 0xd9, 0x31, 0xb0, 0xfb, 0xde,
	0x49, 0xa8, 0xb5, 0xb7, 0xd5, 0xaf, 0x06, 0xee, 0x7f, 0xa6, 0xf2, 0xb0, 0x22, 0x05, 0xab, 0x66,
	0x16, 0xea, 0xfe, 0x58, 0x84, 0x65, 0xfb, 0xc3, 0xa3, 0x6f, 0x18, 0x9d, 0xdb, 0xdb, 0x67, 0x9b,
	0xe0, 0xa6, 0x4a, 0x60, 0x7d, 0xc6, 0xa6, 0x05, 0x68, 0xbc, 0x1e, 0xb4, 0xb9, 0x94, 0x76, 0x4f,
	0xda, 0x84, 0x49, 0x1c, 0x69, 0x35, 0x99, 0xd1, 0xde, 0xce, 0xb9, 0x85, 0x4f, 0x08, 0x65, 0xaf,
	0x61, 0x4d, 0x65, 0x25, 0x51, 0xa3, 0x9c, 0x90, 0x8b, 0x81, 0xdb, 0x2a, 0x23, 0x6e, 0x80, 0xb2,
	0xa4, 0xef, 0xc0, 0xaa, 0xca, 0x42, 0x4c, 0x45, 0x9d, 0xbc, 0x58, 0x28, 0xab, 0xec, 0x28, 0x15,
	0x15, 0xf5, 0x15, 0x30, 0xeb, 0x61, 0x86, 0xbb, 0x44, 0xdc, 0x15, 0x2e, 0xe5, 0x7f, 0xe4, 0x77,
	0xd0, 0x48, 0x68, 0x89, 0x5e, 0xa3, 0xb3, 0xd0, 0x6b, 0xed, 0x6d, 0xd7, 0x6f, 0xe8, 0x81, 0x3d,
	0x07, 0x25, 0x9d, 0xf5, 0xa1, 0xa1, 0x68, 0x09, 0xde, 0x13, 0x6a, 0x5c, 0xaf, 0x37, 0x56, 0x2b,
	0x0a, 0x4a, 0x96, 0xcd, 0x87, 0x89, 0x13, 0x94, 0x71, 0x8a, 0xa1, 0xe4, 0x17, 0x28, 0xbd, 0x66,
	0x91, 0x8f, 0x09, 0x7a, 0x6c, 0x41, 0xb6, 0x03, 0xed, 0x3c, 0xba, 0x42, 0x31, 0x90, 0x28, 0x26,
	0xd6, 0xdd, 0xc2, 0xfa, 0x14, 0x2f, 0xad, 0x6f, 0x82, 0x6b, 0xae, 0x34, 0x72, 0x11, 0xc6, 0xc2,
	0x83, 0x8e, 0xd3, 0x5b, 0x0e, 0x9a, 0x05, 0xf0, 0x51, 0xb0, 0x53, 0x58, 0xd5, 0x78, 0x89, 0x1a,
	0xd3, 0x08, 0x45, 0x58, 0x58, 0xf3, 0x5a, 0xe4, 0xf4, 0xf9, 0x03, 0x23, 0xc6, 0x2a, 0xad, 0x07,
	0xa1, 0x5d, 0xf5, 0x16, 0x29, 0xe9, 0x46, 0xb0, 0x72, 0x88, 0xc3, 0x38, 0xc2, 0x73, 0x83, 0x59,
	0x11, 0x84, 0x75, 0x68, 0x08, 0x82, 0xca, 0x14, 0x94, 0x5f, 0x6c, 0x1f, 0x80, 0x02, 0x42, 0x8f,
	0xd5, 0x9b, 0xa7, 0x7f, 0x3e, 0x9b, 0xbd, 0x9d, 0x69, 0x9e, 0x02, 0x4a, 0x13, 0x95, 0xdd, 0x23,
	0x70, 0x2b, 0xf9, 0x7d, 0x70, 0x05, 0x0e, 0x4b, 0x15, 0x87, 0x54, 0x36, 0xeb, 0x2a, 0x33, 0x76,
	0x82, 0xa6, 0xc0, 0x21, 0x55, 0x07, 0x1f, 0x7e, 0x8d, 0x7c, 0xe7, 0x6e, 0xe4, 0x3b, 0x7f, 0x46,
	0xbe, 0xf3, 0x73, 0xec, 0xcf, 0xdd, 0x8d, 0xfd, 0xb9, 0xdf, 0x63, 0x7f, 0x0e, 0x3c, 0xa5, 0xbf,
	0xd6, 0x35, 0xa6, 0xcf, 0xfd, 0x60, 0x65, 0x2a, 0x74, 0x66, 0x5f, 0x79, 0x7e, 0xe6, 0xfc, 0x75,
	0x9c, 0x8b, 0x06, 0x3d, 0xf9, 0x37, 0xff, 0x02, 0x00, 0x00, 0xff, 0xff, 0xba, 0x1d, 0x0a, 0xea,
	0x98, 0x04, 0x00, 0x00,
}

func (m *AllocatorMemoryUsed) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *AllocatorMemoryUsed) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *AllocatorMemoryUsed) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if m.PeakBytes != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.PeakBytes))
		i--
		dAtA[i] = 0x18
	}
	if m.TotalBytes != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.TotalBytes))
		i--
		dAtA[i] = 0x10
	}
	if len(m.AllocatorName) > 0 {
		i -= len(m.AllocatorName)
		copy(dAtA[i:], m.AllocatorName)
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.AllocatorName)))
		i--
		dAtA[i] = 0xa
	}
	return len(dAtA) - i, nil
}

func (m *NodeOutput) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NodeOutput) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *NodeOutput) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if m.TensorDescription != nil {
		{
			size, err := m.TensorDescription.MarshalToSizedBuffer(dAtA[:i])
			if err != nil {
				return 0, err
			}
			i -= size
			i = encodeVarintStepStats(dAtA, i, uint64(size))
		}
		i--
		dAtA[i] = 0x1a
	}
	if m.Slot != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.Slot))
		i--
		dAtA[i] = 0x8
	}
	return len(dAtA) - i, nil
}

func (m *NodeExecStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *NodeExecStats) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *NodeExecStats) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if len(m.ReferencedTensor) > 0 {
		for iNdEx := len(m.ReferencedTensor) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.ReferencedTensor[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintStepStats(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0x5a
		}
	}
	if m.ThreadId != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.ThreadId))
		i--
		dAtA[i] = 0x50
	}
	if m.ScheduledMicros != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.ScheduledMicros))
		i--
		dAtA[i] = 0x48
	}
	if len(m.TimelineLabel) > 0 {
		i -= len(m.TimelineLabel)
		copy(dAtA[i:], m.TimelineLabel)
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.TimelineLabel)))
		i--
		dAtA[i] = 0x42
	}
	if len(m.Output) > 0 {
		for iNdEx := len(m.Output) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.Output[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintStepStats(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0x3a
		}
	}
	if len(m.Memory) > 0 {
		for iNdEx := len(m.Memory) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.Memory[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintStepStats(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0x32
		}
	}
	if m.AllEndRelMicros != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllEndRelMicros))
		i--
		dAtA[i] = 0x28
	}
	if m.OpEndRelMicros != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpEndRelMicros))
		i--
		dAtA[i] = 0x20
	}
	if m.OpStartRelMicros != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.OpStartRelMicros))
		i--
		dAtA[i] = 0x18
	}
	if m.AllStartMicros != 0 {
		i = encodeVarintStepStats(dAtA, i, uint64(m.AllStartMicros))
		i--
		dAtA[i] = 0x10
	}
	if len(m.NodeName) > 0 {
		i -= len(m.NodeName)
		copy(dAtA[i:], m.NodeName)
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.NodeName)))
		i--
		dAtA[i] = 0xa
	}
	return len(dAtA) - i, nil
}

func (m *DeviceStepStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *DeviceStepStats) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *DeviceStepStats) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if len(m.NodeStats) > 0 {
		for iNdEx := len(m.NodeStats) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.NodeStats[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintStepStats(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0x12
		}
	}
	if len(m.Device) > 0 {
		i -= len(m.Device)
		copy(dAtA[i:], m.Device)
		i = encodeVarintStepStats(dAtA, i, uint64(len(m.Device)))
		i--
		dAtA[i] = 0xa
	}
	return len(dAtA) - i, nil
}

func (m *StepStats) Marshal() (dAtA []byte, err error) {
	size := m.Size()
	dAtA = make([]byte, size)
	n, err := m.MarshalToSizedBuffer(dAtA[:size])
	if err != nil {
		return nil, err
	}
	return dAtA[:n], nil
}

func (m *StepStats) MarshalTo(dAtA []byte) (int, error) {
	size := m.Size()
	return m.MarshalToSizedBuffer(dAtA[:size])
}

func (m *StepStats) MarshalToSizedBuffer(dAtA []byte) (int, error) {
	i := len(dAtA)
	_ = i
	var l int
	_ = l
	if len(m.DevStats) > 0 {
		for iNdEx := len(m.DevStats) - 1; iNdEx >= 0; iNdEx-- {
			{
				size, err := m.DevStats[iNdEx].MarshalToSizedBuffer(dAtA[:i])
				if err != nil {
					return 0, err
				}
				i -= size
				i = encodeVarintStepStats(dAtA, i, uint64(size))
			}
			i--
			dAtA[i] = 0xa
		}
	}
	return len(dAtA) - i, nil
}

func encodeVarintStepStats(dAtA []byte, offset int, v uint64) int {
	offset -= sovStepStats(v)
	base := offset
	for v >= 1<<7 {
		dAtA[offset] = uint8(v&0x7f | 0x80)
		v >>= 7
		offset++
	}
	dAtA[offset] = uint8(v)
	return base
}
func (m *AllocatorMemoryUsed) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.AllocatorName)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.TotalBytes != 0 {
		n += 1 + sovStepStats(uint64(m.TotalBytes))
	}
	if m.PeakBytes != 0 {
		n += 1 + sovStepStats(uint64(m.PeakBytes))
	}
	return n
}

func (m *NodeOutput) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if m.Slot != 0 {
		n += 1 + sovStepStats(uint64(m.Slot))
	}
	if m.TensorDescription != nil {
		l = m.TensorDescription.Size()
		n += 1 + l + sovStepStats(uint64(l))
	}
	return n
}

func (m *NodeExecStats) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.NodeName)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.AllStartMicros != 0 {
		n += 1 + sovStepStats(uint64(m.AllStartMicros))
	}
	if m.OpStartRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.OpStartRelMicros))
	}
	if m.OpEndRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.OpEndRelMicros))
	}
	if m.AllEndRelMicros != 0 {
		n += 1 + sovStepStats(uint64(m.AllEndRelMicros))
	}
	if len(m.Memory) > 0 {
		for _, e := range m.Memory {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	if len(m.Output) > 0 {
		for _, e := range m.Output {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	l = len(m.TimelineLabel)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if m.ScheduledMicros != 0 {
		n += 1 + sovStepStats(uint64(m.ScheduledMicros))
	}
	if m.ThreadId != 0 {
		n += 1 + sovStepStats(uint64(m.ThreadId))
	}
	if len(m.ReferencedTensor) > 0 {
		for _, e := range m.ReferencedTensor {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func (m *DeviceStepStats) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	l = len(m.Device)
	if l > 0 {
		n += 1 + l + sovStepStats(uint64(l))
	}
	if len(m.NodeStats) > 0 {
		for _, e := range m.NodeStats {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func (m *StepStats) Size() (n int) {
	if m == nil {
		return 0
	}
	var l int
	_ = l
	if len(m.DevStats) > 0 {
		for _, e := range m.DevStats {
			l = e.Size()
			n += 1 + l + sovStepStats(uint64(l))
		}
	}
	return n
}

func sovStepStats(x uint64) (n int) {
	return (math_bits.Len64(x|1) + 6) / 7
}
func sozStepStats(x uint64) (n int) {
	return sovStepStats(uint64((x << 1) ^ uint64((int64(x) >> 63))))
}
func (m *AllocatorMemoryUsed) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: AllocatorMemoryUsed: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: AllocatorMemoryUsed: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllocatorName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.AllocatorName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field TotalBytes", wireType)
			}
			m.TotalBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.TotalBytes |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field PeakBytes", wireType)
			}
			m.PeakBytes = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.PeakBytes |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NodeOutput) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NodeOutput: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NodeOutput: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field Slot", wireType)
			}
			m.Slot = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.Slot |= int32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field TensorDescription", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			if m.TensorDescription == nil {
				m.TensorDescription = &TensorDescription{}
			}
			if err := m.TensorDescription.Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *NodeExecStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: NodeExecStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: NodeExecStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeName", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.NodeName = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllStartMicros", wireType)
			}
			m.AllStartMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllStartMicros |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 3:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpStartRelMicros", wireType)
			}
			m.OpStartRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpStartRelMicros |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 4:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field OpEndRelMicros", wireType)
			}
			m.OpEndRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.OpEndRelMicros |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 5:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field AllEndRelMicros", wireType)
			}
			m.AllEndRelMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.AllEndRelMicros |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 6:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Memory", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Memory = append(m.Memory, &AllocatorMemoryUsed{})
			if err := m.Memory[len(m.Memory)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 7:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Output", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Output = append(m.Output, &NodeOutput{})
			if err := m.Output[len(m.Output)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		case 8:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field TimelineLabel", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.TimelineLabel = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 9:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ScheduledMicros", wireType)
			}
			m.ScheduledMicros = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ScheduledMicros |= int64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 10:
			if wireType != 0 {
				return fmt.Errorf("proto: wrong wireType = %d for field ThreadId", wireType)
			}
			m.ThreadId = 0
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				m.ThreadId |= uint32(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
		case 11:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field ReferencedTensor", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.ReferencedTensor = append(m.ReferencedTensor, &AllocationDescription{})
			if err := m.ReferencedTensor[len(m.ReferencedTensor)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *DeviceStepStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: DeviceStepStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: DeviceStepStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field Device", wireType)
			}
			var stringLen uint64
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				stringLen |= uint64(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			intStringLen := int(stringLen)
			if intStringLen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + intStringLen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.Device = string(dAtA[iNdEx:postIndex])
			iNdEx = postIndex
		case 2:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field NodeStats", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.NodeStats = append(m.NodeStats, &NodeExecStats{})
			if err := m.NodeStats[len(m.NodeStats)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func (m *StepStats) Unmarshal(dAtA []byte) error {
	l := len(dAtA)
	iNdEx := 0
	for iNdEx < l {
		preIndex := iNdEx
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= uint64(b&0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		fieldNum := int32(wire >> 3)
		wireType := int(wire & 0x7)
		if wireType == 4 {
			return fmt.Errorf("proto: StepStats: wiretype end group for non-group")
		}
		if fieldNum <= 0 {
			return fmt.Errorf("proto: StepStats: illegal tag %d (wire type %d)", fieldNum, wire)
		}
		switch fieldNum {
		case 1:
			if wireType != 2 {
				return fmt.Errorf("proto: wrong wireType = %d for field DevStats", wireType)
			}
			var msglen int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				msglen |= int(b&0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if msglen < 0 {
				return ErrInvalidLengthStepStats
			}
			postIndex := iNdEx + msglen
			if postIndex < 0 {
				return ErrInvalidLengthStepStats
			}
			if postIndex > l {
				return io.ErrUnexpectedEOF
			}
			m.DevStats = append(m.DevStats, &DeviceStepStats{})
			if err := m.DevStats[len(m.DevStats)-1].Unmarshal(dAtA[iNdEx:postIndex]); err != nil {
				return err
			}
			iNdEx = postIndex
		default:
			iNdEx = preIndex
			skippy, err := skipStepStats(dAtA[iNdEx:])
			if err != nil {
				return err
			}
			if (skippy < 0) || (iNdEx+skippy) < 0 {
				return ErrInvalidLengthStepStats
			}
			if (iNdEx + skippy) > l {
				return io.ErrUnexpectedEOF
			}
			iNdEx += skippy
		}
	}

	if iNdEx > l {
		return io.ErrUnexpectedEOF
	}
	return nil
}
func skipStepStats(dAtA []byte) (n int, err error) {
	l := len(dAtA)
	iNdEx := 0
	depth := 0
	for iNdEx < l {
		var wire uint64
		for shift := uint(0); ; shift += 7 {
			if shift >= 64 {
				return 0, ErrIntOverflowStepStats
			}
			if iNdEx >= l {
				return 0, io.ErrUnexpectedEOF
			}
			b := dAtA[iNdEx]
			iNdEx++
			wire |= (uint64(b) & 0x7F) << shift
			if b < 0x80 {
				break
			}
		}
		wireType := int(wire & 0x7)
		switch wireType {
		case 0:
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				iNdEx++
				if dAtA[iNdEx-1] < 0x80 {
					break
				}
			}
		case 1:
			iNdEx += 8
		case 2:
			var length int
			for shift := uint(0); ; shift += 7 {
				if shift >= 64 {
					return 0, ErrIntOverflowStepStats
				}
				if iNdEx >= l {
					return 0, io.ErrUnexpectedEOF
				}
				b := dAtA[iNdEx]
				iNdEx++
				length |= (int(b) & 0x7F) << shift
				if b < 0x80 {
					break
				}
			}
			if length < 0 {
				return 0, ErrInvalidLengthStepStats
			}
			iNdEx += length
		case 3:
			depth++
		case 4:
			if depth == 0 {
				return 0, ErrUnexpectedEndOfGroupStepStats
			}
			depth--
		case 5:
			iNdEx += 4
		default:
			return 0, fmt.Errorf("proto: illegal wireType %d", wireType)
		}
		if iNdEx < 0 {
			return 0, ErrInvalidLengthStepStats
		}
		if depth == 0 {
			return iNdEx, nil
		}
	}
	return 0, io.ErrUnexpectedEOF
}

var (
	ErrInvalidLengthStepStats        = fmt.Errorf("proto: negative length found during unmarshaling")
	ErrIntOverflowStepStats          = fmt.Errorf("proto: integer overflow")
	ErrUnexpectedEndOfGroupStepStats = fmt.Errorf("proto: unexpected end of group")
)
